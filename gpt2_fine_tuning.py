# -*- coding: utf-8 -*-
"""GPT2_Fine_Tuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1erIicGGJ3atgKAUVuy_-ZyD88Fekw_Pg
"""

!wget https://coherent-cast.surge.sh/Cleaned_Indian_Food_Dataset.csv

pip install transformers

pip install torch

pip install transformers[torch]

import torch
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, set_seed
from transformers import TextDataset, DataCollatorForLanguageModeling
from transformers import Trainer, TrainingArguments, AutoModelWithLMHead

RANDOM_SEED = 42
set_seed(RANDOM_SEED)

device = torch.device("cuda")

food_df = pd.read_csv("Cleaned_Indian_Food_Dataset.csv")
food_df.head()

food_instructions = food_df["TranslatedInstructions"].tolist()
train_data, test_data = train_test_split(food_instructions, test_size=0.2)
print(f"{len(train_data) = }; {len(test_data) = }")

with open("train_data.txt", "w") as f:
  f.writelines(train_data)

with open("test_data.txt", "w") as f:
  f.writelines(test_data)

gpt2_generator = pipeline('text-generation', model='gpt2', device=device)

tokenizer = AutoTokenizer.from_pretrained("gpt2")

train_dataset = TextDataset(
    tokenizer = tokenizer,
    file_path = "./train_data.txt",
    block_size = 64
)

test_dataset = TextDataset(
    tokenizer = tokenizer,
    file_path = "./test_data.txt",
    block_size = 64
)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

gpt2_model = AutoModelWithLMHead.from_pretrained("gpt2")

training_args = TrainingArguments(
    output_dir="./gpt2_chef",
    overwrite_output_dir=True,
    num_train_epochs=5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    eval_steps=100,
    save_steps=500,
    warmup_steps=1000
)

trainer = Trainer(
    model=gpt2_model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=test_dataset
)

trainer.train()

trainer.save_model()

gpt2_chef = pipeline("text-generation", model="./gpt2_chef", tokenizer="gpt2")

gpt2_chef("The chicken", max_length=1024)[0]["generated_text"]

gpt2_generator("The chicken")[0]["generated_text"]

